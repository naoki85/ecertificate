---
layout: post
title:  "情報量"
date:   2023-07-03 00:44:00 +0900
categories: entropy
---

# 情報量

[情報量](https://ja.wikipedia.org/wiki/情報量#:~:text=情報量（じょうほうりょう,を表す尺度である。)

> 情報量やエントロピーは、情報理論の概念で、ある事象が起きた際、それがどれほど起こりにくいかを表す尺度である。

## 情報量

$$
I(E) = -logP(E)
$$

## エントロピー（平均情報量）

$$
H(X) = -\Sigma p(x)logP(X)
$$

## 交差エントロピー

[交差エントロピー](https://ja.wikipedia.org/wiki/交差エントロピー)

$$
H(p, q) = -\Sigma p(x)log(q(x))
$$

## KLダイバージェンス

KLダイバージェンスとは2つの確率分布の異なりを数値化したもの。  
大きい方が確率の異なりも大きく、小さければ小さいほど似た確率となる。  
確率を表す2つの連続分布関数 p(x) 、 q(x) が存在するとき、次のような期待値を **KLダイバージェンス** と呼ぶ。
  

$$
D_{KL}[p(x)][q(x)] = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx = \int_{-\infty}^{\infty} p(x) \log p(x) dx - \int_{-\infty}^{\infty} p(x) \log q(x) dx
$$
  

KLダイバージェンスには任意の確率分布の組み合わせに対して $$KL[p(x)][q(x)] \ge 0$$ という性質がある  
確率の異なりを表したものであるため、 $$KL[p(x)][q(x)] = 0$$ となり等号が成り立つのは2つの分布が完全に一致する場合に限られる。  
KLダイバージェンスは非負であり2つの確率分布の「異なり」を数値化していることから、2つの分布間のなんらかの距離を表すと考えられがち。  
  
しかし、一般的には $$KL[p(x)][q(x)] \neq KL[q(x)][p(x)]$$ となる。  
$$p(x)$$ を基準にするか $$q(x)$$ を基準にするかで、KLダイバージェンスの値は変化してしまう。  

## JSダイバージェンス

そこで、KLダイバージェンスに対称性を持たせた、JSダイバージェンスと呼ばれる指標が利用されている。  
特に、2014年に発表された GAN の最適化の議論に用いられたことで、注目を集めた。  
  
JSダイバージェンスは、以下のようにKLダイバージェンスを用いて求められる。  

$$
D_{JS} = \frac{1}{2} D_{KL}[p(x)][m(x)] + \frac{1}{2} D_{KL}[q(x)][m(x)]
$$

ここで、$$m(x)$$ は2つの確率分布の平均をとった分布。  
  
$$
m(x) = \frac{p(x) + q(x)}{2}
$$
  
また、JSダイバージェンスには対称性があるため、以下の等式が成り立つ。

$$
D_{JS}[p(x)][q(x)] = D_{JS}[q(x)][p(x)]
$$

そのため、任意の2つの確率分布の「異なり」をJSダイバージェンスによって測る際には、どちらの分布を基準にして測ったとしても、同様の値が得られることとなる。

## モンテカルロ積分

[モンテカルロ積分の直感的理解と必要サンプル数の導出・精度向上などまとめ - あつまれ統計の森](https://www.hello-statisticians.com/explain-terms-cat/monte_carlo1.html){:target="blank"}  
真の分布 p(x) による期待値をデータ D による平均によって置き換える。  
以下のようなエントロピーがあったとき、

$$
H(X) = -\int_X p(x) \log q(x; \theta)
$$

以下のように置き換えることができる。

$$
\tilde{H(X)} = - \frac{1}{n} \sum_{i=1}^{n} \log q(x; \theta)
$$
