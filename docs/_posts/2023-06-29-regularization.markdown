---
layout: post
title:  "正則化"
date:   2023-06-29 23:19:05 +0900
categories: regularization
---

機械学習における正則化とは、過学習を防ぐためにパラメータの値を制限する手法です。  

# ノルム

ノルムは、機械学習においてよく用いられるベクトルの大きさを表す計算方法です。  
L1 ノルムと L2 ノルムが使用されます。

## L1 ノルム

L1 ノルムは、各要素の絶対値の和を表します。  
例えば、ベクトル x = `[1, -2, 3]` の L1 ノルムは、  
$$
|1| + |-2| + |3| = 6
$$

L1 ノルムは、ラッソ回帰による正則化に使用されます。  
ラッソ回帰では、コスト関数に L1 ノルムを加えることで、モデルのパラメータを 0 に近づけることができます。

## L2 ノルム

L2 ノルムは、各要素の二乗和の平方根を表します。  
例えば、ベクトル x = `[1, -2, 3]` の L2 ノルムは、  
$$
\sqrt{(1^2 + (-2)^2 + 3^2)}
$$

L2 ノルムは、リッジ回帰による正則化に使用されます。  
リッジ回帰では、コスト関数に L2 ノルムを加えることで、モデルのパラメータを大きくなりすぎないように制限することができます。

# 正則化手法

リッジ回帰、ラッソ回帰、エラスティックネットは、過学習を防ぐために重要な役割を果たします。  
正則化の対象とするパラメータにはバイアスを含まないことが一般的です。  
重みと異なり、バイアスが大きくなっても過剰適合に繋がることが少ないためです。

## リッジ回帰

リッジ回帰は、コスト関数に L2 ノルムを加えることで実現されます。  
この手法により、パラメータの値が大きくなりすぎないように制限されます。

## ラッソ回帰

ラッソ回帰は、コスト関数に L1 ノルムを加えることで実現されます。
この手法により、一部のパラメータが 0 になることがあります。

## エラスティックネット

エラスティックネットは、リッジ回帰とラッソ回帰を組み合わせた手法です。  
コスト関数に L1 ノルムと L2 ノルムの和を加えることで実現されます。  
この手法により、リッジ回帰とラッソ回帰の両方の効果を得ることができます。

# 参考

- [ラッソ回帰とリッジ回帰の理論 - Qiita](https://qiita.com/oki_kosuke/items/fb8bb418167f2ab1744e)
- [リッジ回帰(L2正則化)を理解して実装する - Qiita](https://qiita.com/g-k/items/d3124eb00cb166f5b575)