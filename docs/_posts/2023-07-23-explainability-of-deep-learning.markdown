---
layout: post
title:  "深層学習の説明性"
date:   2023-07-23 23:38:00 +0900
categories: explainability-of-deep-learning
---

# 万能近似定理

この定理は、特定の条件下で、ニューラルネットワークが任意の連続関数を任意の精度で近似できることを示している。  
具体的には、隠れ層が1つで、その層のニューロン数が十分に多い場合、そして活性化関数が非線形である場合、ニューラルネットワークは任意の連続関数を近似できるというもの。

## 定理の詳細

- **関数の近似**: 万能近似定理は、多層パーセプトロン（MLP）が、与えられたコンパクトな集合上の任意の連続関数を任意の精度で近似できることを示している。
- **隠れ層の重要性**: 定理は、隠れ層が1つであれば十分であることを示している。ただし、その隠れ層のニューロン数が十分に多い必要がある。
- **活性化関数**: この定理が成り立つためには、活性化関数が非線形である必要がある。一般的にシグモイド関数やReLU（Rectified Linear Unit）などが使用される。

## 制限と注意点

- この定理は、ニューラルネットワークが関数を近似できることを保証するが、そのような近似が訓練データから学習できるとは限らない。
- 実際の応用では、適切なネットワーク構造を見つけ、訓練データで効果的に学習するための適切な訓練手法を選ぶことが重要。
- また、隠れ層が1つであれば理論的には十分ですが、実際の問題では、多くの隠れ層を持つ深いネットワークが必要な場合もある。このような深いネットワークは、表現力が高く、より効率的に学習できることが多い。

# SHAP (SHapley Additive exPlanations)

ある入力に対しての予測結果に対して、どの特徴量が寄与したのかを解析する手法。  
LIME と同様、局所的説明である。  

SHAP では入力 $$x = [x_1, x_2, ...x_m]^T$$ と学習されたモデル f が与えられた時、モデル f を各変数の寄与度が説明しやすい簡単なモデルで近似する。

$$
g(z') = \phi_0 + \sum_{j=1}^{M} \phi_j z_i'
z' = [z_1', ..., z_M']
$$

ここで、入力 xを単純化した z′ を考える。
各 $$z_i'$$ は例えば x の i 番目の変数が観測されていれば 1 、そうでなければ 0 となる。
今求めたい変数の寄与度は上式での $$\phi_i$$ 。  
  
SHAPではモデル g に対して次の性質を持つように制約を加えます。  
  
- local accuracy: 学習済みモデル f で予測した結果 f(x) とモデル g で予測した結果 g(z) が一致、つまり $$\phi_i$$ の和は説明したいモデルの出力値 f(x) に等しい。
- missingness: $$z_i' = 0$$ のときは $$\phi_i = 0$$ 。つまり、結果に影響を与えないような特徴量は、その予測に対して貢献していない。
- consistency: ある変数のモデル f の出力に対する影響力が大きければ、その変数の寄与は大きくなる（ $$\phi_i = 0$$ が大きくなる）