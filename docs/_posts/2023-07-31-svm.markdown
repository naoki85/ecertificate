---
layout: post
title:  "SVM"
date:   2023-07-31 21:14:00 +0900
categories: svm
---

# SVM

SVM (Support Vector Machine) は、機械学習において広く使用される分類器の一つです。SVM は、線形分類問題に対して高い分類性能を発揮しますが、線形分離不可能な問題に対しては、カーネルトリックを使用することで解決することができます。

## 線形分類問題

線形分類問題は、2つのクラスに属するデータが与えられたときに、そのデータを2つのクラスに分類する問題です。線形分類問題では、2つのクラスを分離する直線や平面を見つけることが目的です。

## 線形分離不可能な問題

しかし、現実の問題には線形分離不可能な問題も存在します。線形分離不可能な問題は、2つのクラスを分離するための直線や平面を見つけることができない問題です。例えば、以下のような問題が線形分離不可能な問題に当たります。

線形分離不可能な問題を解決するためには、カーネルトリックを使用することができます。

## カーネルトリック

カーネルトリックとは、非線形分類問題を解くために、データを高次元空間に写像することで、線形分離可能な問題にする手法です。カーネルトリックを使用することで、高次元空間での計算を低次元空間で行うことができます。

具体的には、カーネル関数を使用して、元の特徴空間から別の特徴空間に写像します。一般的に使用されるカーネル関数には、多項式カーネル、ガウスカーネル、シグモイドカーネルなどがあります。

カーネルトリックを使用したSVMの最適化問題は、次のように表されます。

$$
\min \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i,x_j)-\sum_{i=1}^n \alpha_i
$$

制約条件は、

$$
\sum_{i=1}^n \alpha_i y_i=0
$$

$$
0 \leq \alpha_i \leq C
$$

です。ここで、 $\alpha_i$ はラグランジュ乗数、 $y_i$ はクラスラベル、 C はハイパーパラメータです。

## 動的基底関数

動的基底関数は、カーネル関数を使用する代わりに、データを動的に変換する手法です。動的基底関数には、ニューラルネットワークを使用する方法があります。ニューラルネットワークは、非線形変換を行うことができるため、カーネルトリックと同様に線形分離不可能な問題を解決することができます。

以上が、SVM についての説明です。SVM は、線形分類問題に対して高い分類性能を発揮し、線形分離不可能な問題に対してもカーネルトリックを使用することで解決することができます。

# ソフトマージンSVM

ソフトマージンSVMは、線形分離不可能な問題に対して、ハードマージンSVMよりも柔軟な対応が可能な手法です。

ハードマージンSVMでは、訓練データが線形分離可能であることが前提とされていました。しかし、現実のデータはノイズや外れ値を含むことがあり、線形分離不可能な場合が多いです。

ソフトマージンSVMは、ハードマージンSVMと異なり、誤分類を許容することができます。つまり、訓練データを完全に分離することができなくても、一定の誤分類を許容して最適な決定境界を求めることができます。

ソフトマージンSVMの最適化問題は、次のように表されます。

$$
\min \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i,x_j)-\sum_{i=1}^n \alpha_i
$$

制約条件は、

$$
\sum_{i=1}^n \alpha_i y_i=0
$$

$$
0 \leq \alpha_i \leq C
$$

です。ここで、$\alpha_i$はラグランジュ乗数、$y_i$はクラスラベル、$C$はハイパーパラメータです。

## ハードマージンSVM

ハードマージンSVMは、線形分類問題に対して、最適な分離超平面を求める手法です。

ハードマージンSVMの最適化問題は、次のように表されます。

$$
\min \frac{1}{2}\left\|w\right\|^2
$$

制約条件は、

$$
y_i(w^Tx_i+b) \geq 1
$$

です。ここで、$w$は分離超平面の法線ベクトル、$b$はバイアス項、$y_i$はクラスラベルです。

ハードマージンSVMは、訓練データが線形分離可能であることが前提となります。しかし、現実のデータはノイズや外れ値を含むことがあり、線形分離不可能な場合が多いです。

ソフトマージンSVMは、ハードマージンSVMよりも柔軟な対応が可能です。誤分類を許容することができるため、線形分離不可能な問題にも対応することができます。

以上が、ソフトマージンSVMとハードマージンSVMについての説明です。

SVM において、線形分離不可能な問題を解決するために、ソフトマージンSVM が使用されます。ソフトマージンSVM は、誤分類を許容することができます。

ソフトマージンSVM では、誤分類されたデータに対して、罰金を課すことで、誤分類を最小化します。この罰金をスラック変数と呼びます。

スラック変数 $\xi$ は、以下のように定義されます。

$$\xi_i=\max(0,1-y_i(w^Tx_i+b))$$

ここで、$w$ は分離超平面の法線ベクトル、$b$ はバイアス項、$y_i$ はクラスラベルです。スラック変数 $\xi_i$ は、$i$ 番目のデータが誤分類された場合に、そのデータに対して課される罰金を表します。

スラック変数を導入したソフトマージンSVM の最適化問題は、次のように表されます。

$$
\min \frac{1}{2}\left\|w\right\|^2+C\sum_{i=1}^n \xi_i
$$

制約条件は、

$$y_i(w^Tx_i+b) \geq 1-\xi_i$$

$$\xi_i \geq 0$$

です。ここで、$C$ はハイパーパラメータであり、スラック変数に対する罰金の度合いを調整します。

以上が、ソフトマージンSVM で使用されるスラック変数についての説明です。

# SVMでカーネルトリックを使用した場合の線形分離不可能な問題の解決方法

SVMは、線形分類問題に対して高い分類性能を発揮しますが、線形分離不可能な問題に対しては、カーネルトリックを使用することで解決することができます。

カーネルトリックとは、非線形分類問題を解くために、データを高次元空間に写像することで、線形分離可能な問題にする手法です。カーネルトリックを使用することで、高次元空間での計算を低次元空間で行うことができます。

具体的には、カーネル関数を使用して、元の特徴空間から別の特徴空間に写像します。一般的に使用されるカーネル関数には、多項式カーネル、ガウスカーネル、シグモイドカーネルなどがあります。

カーネルトリックを使用したSVMの最適化問題は、次のように表されます。

$$
\min \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i,x_j)-\sum_{i=1}^n \alpha_i
$$

制約条件は、

$$
\sum_{i=1}^n \alpha_i y_i=0
$$

$$
0 \leq \alpha_i \leq C
$$

です。ここで、\alpha_i はラグランジュ乗数、 $y_i$ はクラスラベル、C はハイパーパラメータです。

動的基底関数は、カーネル関数を使用する代わりに、データを動的に変換する手法です。動的基底関数には、ニューラルネットワークを使用する方法があります。ニューラルネットワークは、非線形変換を行うことができるため、カーネルトリックと同様に線形分離不可能な問題を解決することができます。

以上が、SVMでカーネルトリックを使用した場合の線形分離不可能な問題の解決方法と、動的基底関数についての説明です。