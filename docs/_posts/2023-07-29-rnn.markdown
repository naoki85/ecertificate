---
layout: post
title:  "回帰型ニューラルネットワーク"
date:   2023-07-29 00:04:00 +0900
categories: rnn
---

# RNN

## 双方向RNN (Bidirectional Recurrent Neural Network) 

順方向のRNNと逆方向のRNNを組み合わせた形のニューラルネットワーク。  
これにより、過去の情報だけでなく未来の情報も使用することができる。  
  
主な特性は、

- 順方向と逆方向の情報の統合
- 双方向 RNN だけでは長期のデータには適応できない。
- 計算コストと複雑さの増加

## ゲート付きリカレントニューラルネットワーク（Gated Recurrent Neural Networks）

長期的な依存関係を学習するためにゲートメカニズムを導入したリカレントニューラルネットワーク（RNN）の一種。
代表的なゲート付き RNN は、 LSTM と GRU の 2 種類。

## LSTM

[LSTM (Long Short-Term Memory)](https://cvml-expertguide.net/terms/dl/rnn/lstm/)  
LSTMは、 **入力ゲート** 、 **忘却ゲート** 、 **出力ゲート** と呼ばれる3つのゲートと、セル状態と呼ばれる内部の状態を持つ。  
これにより、LSTMは長期的な依存関係を捉え、情報を長期にわたって保持する能力を持つ。

### 入力ゲート

$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

### 忘却ゲート

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

### 出力ゲート

$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$

### セル状態

$$
g_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
$$

$$
c_t = f_t \cdot c_{t-1} + i_t \cdot g_t
$$

### 隠れ状態

$$
h_t = o_t \cdot \tanh(c_t)
$$

tahn 計算を行うのは、新しい記憶せるを作成する際に前の隠れ層と入力を処理する場合と、新しい隠れ層を作成するとき。

## GRU

[GRU (Gated Recurrent Unit, ゲート付き再帰ユニット)](https://cvml-expertguide.net/terms/dl/rnn/gru/)  
GRUはLSTMの簡易版であり、 **リセットゲート** と **更新ゲート** の2つのゲートを持つ。  
GRUはLSTMと比べてパラメータが少ないため、計算効率が向上し、データが少ない場合にも適している。  
LSTM に比べ、GRUは **隠れ状態** だけを使用して情報を保存する。

### リセットゲート

$$
r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
$$

### 更新ゲート

$$
z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
$$

### 候補隠れ状態

$$
h_t' = \tanh(W \cdot [r_t \cdot h_{t-1}, x_t] + b)
$$

### 隠れ状態

$$
h_t = (1 - z_t) \cdot h_{t-1} + z_t \cdot h_t'
$$

tanh 計算をするには隠れ層の候補状態を出す時だけで、他は sigmoid 計算になる。

ここで、\(W_r\), \(W_z\), \(W\) および \(b_r\), \(b_z\), \(b\) はパラメータで、\([r_t \cdot h_{t-1}, x_t]\)はリセットゲートによって調整された前の隠れ状態と現在の入力を連結したものを示しています。

## エコーステートネット（Echo State Network、ESN）

リカレントニューラルネットワーク（RNN）の一種で、特にリザーバーコンピューティングと呼ばれるフレームワークの一部として知られている。  
エコーステートネットワークは、以下の2つの主要なコンポーネントで構成されている。

1. リザーバー：リザーバーは大規模でスパース（疎）なRNNで、非常にランダムな方法で接続されています。リザーバーのユニークな特性は、重みが初期化後に固定され、訓練中には更新されないことです。これにより、リザーバーはダイナミックな「エコーステート」を生成し、入力の時間的な特性をキャプチャします。
2. リードアウト層：リードアウト層はリザーバーからの活動を読み取り、出力を生成する。重要なのは、リードアウト層の重みのみが訓練され、リザーバーの重みは固定されているという点です。

エコーステートネットワークの主な利点は、訓練が非常に簡単で効率的なこと。  
これは、学習すべき重みがリードアウト層に限定されているためであり、そのため一般的な線形回帰手法を使用して効率的に最適化することが可能。  
一方で、リザーバーの性能は初期重みやリザーバーの規模などのハイパーパラメータに強く依存する。  
これらのパラメータを適切に設定することは、モデルの成功にとって重要な要素となる。  
ESNは一部のタスク、特に時系列予測や一部の動的システムのモデリングにおいて有効であると実証されているが、その一方で深層学習に比べて一般化能力が低いという課題もある。  
ESN では入力、隠れ層の重みはランダムに固定し、出力層の重みを学習する  
  
[https://qiita.com/donaldchi/items/c65b978de384821f2942](https://qiita.com/donaldchi/items/c65b978de384821f2942)

# Attention, Transformer

Attentionは、 Query と Key から重みを計算し、 Value の重み付き平均を出力するもの。  
encoder-decoderモデルで用いられる source-target-Attention では Key と Value は encoder から、 Query は decoder から計算される。  

$$
Attention(Q, K, V) = softmax(QK^T) \cdot V
$$

Query 、 Key の次元 ( $$d_k$$ ) が大きくなると逆伝播時のソフトマックス関数の勾配が小さくなり、学習が円滑に進まなくなる。  
そのため、 $$d_k$$ の平方根で除算する。  

$$
Attention(Q, K, V) = softmax( \frac{QK^T}{\sqrt{d_k}} ) \cdot V
$$

## ポジショナルエンコーディング

$$
PE_{(pos, 2i)} = \sin(\frac{pos}{10000^{2i/d_{model}}})
$$

$$
PE_{(pos, 2i+1)} = \cos(\frac{pos}{10000^{2i/d_{model}}})
$$

# BERT

事前学習では、マスクされる単語を予測する問題と、次の文章かどうかを予測する問題を同時に解く。  
これはコーパスデータを使用した教師なし学習によって行われる。  
再学習では事前学習したパラメータを初期値として、何らかの解きたいタスクを解く。  
再学習は教師あり学習によって行われる。

# BLUE スコア

[【自然言語処理】BLEU - 定義は？どういう意味？何で利用されてるの？【機械翻訳の評価指標】](https://youtu.be/aZJAizFSTWg)  
  
機械翻訳文が人の作成した文にどれだけ近いか評価する客観的指標。

$$
BLEU = e^{min(0, 1 - \frac{r}{c})} exp ( \sum_{n=1}^N w_n log p_n )
$$
