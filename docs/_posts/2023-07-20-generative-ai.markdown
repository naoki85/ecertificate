---
layout: post
title:  "生成モデル"
date:   2023-07-20 08:03:00 +0900
categories: generative-ai
---

# 識別関数、識別モデル、生成モデル

識別関数とは、データ x が入力された際に、そのデータが属するクラスの番号を直接出力する関数。  
識別関数は入力空間に、以下の図のように識別超平面を引き、その平面によって分類を行う。  
実装が比較的容易な反面、どれくらい離れているのかがわかりにくい。  
  
識別モデルとは、サンプルデータ x がクラス $$y_k$$ に所属している確率 $$p(y_k|x)$$ を直接構成するモデル。  
つまり、どのくらい確からしいかがわかる。ただし、実装は複雑になる。  
  
生成モデルとは観測データの分布を確率分布として観測データから推定するモデル。  
識別モデルと同様に、条件付き確率 $$p(y_k|x)$$ を予測しますが、その方法が異なる。  
学習によって取得した確率分布に従って、サンプルの生成が可能になる。

例としては、

$$
p(y|x) =\frac{p(x|y)p(y)}{p(x)}
$$

で、識別モデルでは左辺を予測したが、生成モデルでは右辺を予測する。  
p(x) は p(y|x) の最終出力には寄与しないので、 $$y = argmax_y p(x|y)p(y)$$ を求める。

# GAN (Generative Adversarial Network)

- 生成器と識別器が存在する。
- 生成器は本物に近い画像を生成できるよう学習する。識別器は本物と偽物を区別できるように学習する。

GAN の目的関数は下の式。

$$
min_G max_D V(G, D) = E_(x∼p_{data})[logD(x)]+E_(x∼p_z)[log(1 - D(G(z)))]
$$

前項は識別器が本物を見分ける能力。  
G(z) は生成器が偽物を生成する能力なので、 D(G(z)) は偽物を本物と勘違いする確率。  
つまり、 1-D(G(z)) は偽物を偽物と見抜く能力と考えられる。  
  
この目的関数において、まずは識別器の D を最大化するよう学習し、偽物を本物とするように G に関して最小化するよう学習する。

学習が進み、生成器と識別器のそれぞれが理想的な性能を有するようになった場合、生成器の生成するデータ分布と訓練データの分布が一致するため、 0.5 に収束する。

# 変分自己符号化器（Variational Autoencoder、VAE）

VAE は、深層学習とベイズ推論を組み合わせた一種の生成モデル。  
従来の自己符号化器（Autoencoder）は入力データを低次元の潜在空間にエンコード（符号化）し、その後デコード（復号化）することで元のデータを再構成します。目標は、エンコードとデコードを通じて元の入力をできるだけ正確に再現することだった。  
この過程を通じて、モデルはデータの重要な特徴を捉える能力を獲得する。  
  
VAE はこの概念をさらに進め、潜在空間における確率的表現を学習する。  
つまり、VAEは各データポイントを特定の潜在ベクトルにエンコードするのではなく、潜在ベクトルが従う確率分布（通常はガウス分布）のパラメータを推定する。  

そのため、VAEはデータの生成モデルとして使用できる。  
新たなデータポイントを生成するために、まず潜在空間からランダムなベクトル（ガウス分布からのサンプリング）を選び、それをデコーダに渡して新たなデータを生成する。  
サンプリングしたものを隠れ変数と呼ばれる。 $$z~N(\mu, \sigma^2)$$  
隠れ変数はそのままだと正常に学習できないので、以下のように近似する。  

$$
z = \mu + \varepsilon \sigma
$$

また、VAEは損失関数として再構成損失とKLダイバージェンスという2つの項を考慮する。  
再構成損失は元のデータと再構成データの間の差を計測する。  
一方、KLダイバージェンスは学習した確率分布と事前分布（通常は標準正規分布）との間の差を計測する。  
この二つのバランスが、VAEが有効に潜在空間を学習する上で重要となる。
VAEは、画像生成、テキスト生成、推薦システム、画像の欠損値補完など、様々な応用がある。