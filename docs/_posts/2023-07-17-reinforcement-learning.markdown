---
layout: post
title:  "強化学習"
date:   2023-07-17 14:45:00 +0900
categories: reinforcement-learning
---

# 強化学習

強化学習は、機械学習の一種であり、エージェントと呼ばれるものが行動を行い、その結果として報酬を得ることで学習を進めていく手法です。  
エージェントは状態を観測し、その状態に応じてアクションを選択します。  
そのアクションによって状態が変化し、報酬が与えられます。  
この報酬によって、エージェントは良い行動と悪い行動を判断し、良い行動を取るように学習を進めます。
強化学習は、様々な分野で活用されています。例えば、ゲームのAI、ロボットの制御、自動運転車の開発などが挙げられます。強化学習は、人間が行うような制御や意思決定を自律的に行うことができるため、今後ますます注目を集める技術となっていくでしょう。

- [マルコフ決定過程](https://ja.wikipedia.org/wiki/マルコフ決定過程)
- [今さら聞けない強化学習（1）：状態価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)

## マルコフ決定過程

マルコフ決定過程(Markov Decision Process, MDP)は、強化学習における数理モデルです。  
MDPは、エージェントがある状態から別の状態に移行するためのアクションを選択することで得られる報酬を最大化するように学習する問題を表現しています。  
MDPは、状態、アクション、報酬、遷移確率、割引率といった要素から構成されます。

### 状態

状態(state)とは、エージェントが存在する環境の状態を表します。  
例えば、迷路の中でエージェントがどの位置にいるか、といった情報が状態となります。  
MDPでは、状態は離散的な値で表現されます。

### アクション

アクション(action)とは、エージェントがとることのできる行動を表します。  
例えば、迷路の中でエージェントが上下左右に動くことができる場合、上、下、左、右の4つのアクションが存在します。  
MDPでは、アクションも離散的な値で表現されます。

### 報酬

報酬(reward)とは、エージェントがアクションを実行した結果として得られる値を表します。  
MDPでは、報酬は状態とアクションに依存する値であり、エージェントが目指すべき目的を表すとも言えます。

### 遷移確率

遷移確率は、現在の状態とアクションによって、次の状態に遷移する確率を表します。  
MDPでは、遷移確率は状態とアクションに依存する確率分布で表現されます。

### 割引率

割引率(discount factor)とは、将来得られる報酬の重要度を表す値です。  
MDPでは、時間の経過に伴って報酬が減衰することがあるため、割引率を用いて将来の報酬を現在の価値に換算します。

## 動的計画法（DP 法）

動的計画法は、状態価値関数を用いて、最適な行動を選択するための手法です。  
状態価値関数とは、ある状態における価値を表す関数です。  
エージェントは、状態価値関数を更新しながら、最適な行動を選択していきます。  
動的計画法は、状態数が限られている場合に有効な手法です。  
しかし、状態数が膨大な場合には計算量が膨大になるため、実用的ではありません。

## モンテカルロ法

モンテカルロ法は、エージェントがエピソードを繰り返しながら、報酬を得ることで問題を解決する手法です。  
エピソードとは、ある状態から始まり、終了条件が満たされるまで行動を繰り返し、最終的に報酬を得るまでの一連の流れのことです。  
モンテカルロ法は、エピソードの集合から状態価値関数を推定します。  
モンテカルロ法は、動的計画法と比較して、状態数が膨大な場合でも適用できるため、実用的な手法として広く使われています。

## TD学習

TD学習は、エージェントが報酬を得ながら、状態価値関数を更新する手法です。  
TD学習は、モンテカルロ法と動的計画法の両方の長所を取り入れた手法であり、エピソードが不要でありながら、状態数が膨大な場合でも適用できるため、実用的な手法として広く使われています。

### 参考

- [強化学習の基礎 - Qiita](https://qiita.com/icoxfog417/items/92ac72d4f4463f464fc3)
- [今さら聞けない強化学習（10）: SarsaとQ学習の違い - Qiita](https://qiita.com/triwave33/items/cae48e492769852aa9f1)
- [強化学習の基礎知識とアルゴリズムについて - RICOH](https://www.ricoh.co.jp/technology/techreport/2018/201811/pdf/002.pdf)

## SarsaとQ学習の違い

SarsaとQ学習は、強化学習のアルゴリズムの一種であり、状態行動価値関数を求めるために用いられます。  
Sarsaは、ペアワイズで学習を行い、Q学習は、ベストな行動（ベルマン最適関数）に基づいて学習を行うという違いがあります。
Sarsaは、現在の状態と行動に対する価値を更新する際に、次の行動を決定するために、ε-greedy方策を採用します。  
つまり、εの確率でランダムな行動をとり、1-εの確率で最適な行動をとります。  
これにより、探索と利用のバランスを取ることができます。
  
また、Q 学習に比べて行動価値の小さい探索結果が反映されやすい。計算が不安定になりやすい。
一方、Q学習は、現在の状態において、最大の行動価値を持つ行動をとります。  
つまり、常に最適な行動をとることを目指します。  
そのため、探索に偏った学習を行うことができず、最適解に収束するまでに時間がかかることがあります。
  
SarsaとQ学習は、それぞれの特徴に応じて、問題に応じて選択することができます。  
Sarsaは、探索と利用のバランスを取りながら、最適解に収束することができます。  
一方、Q学習は、最適解に収束するまでに時間がかかることがありますが、最終的には最適解を得ることができます。

### 参考

- [今さら聞けない強化学習（10）: SarsaとQ学習の違い - Qiita](https://qiita.com/triwave33/items/cae48e492769852aa9f1)

## 方策勾配法

方策勾配法（Policy Gradient Method）は、強化学習における方策関数の最適化手法です。  
方策関数は、現在の状態に応じて、エージェントがどのような行動を取るかを決定する関数です。  
方策勾配法は、方策関数を微小に変更し、報酬を最大化するように更新していきます。  
  
方策勾配法では、方策関数の勾配を計算し、その勾配に従って方策関数を更新します。  
勾配は、報酬と方策関数の微分によって計算されます。  
方策勾配法では、報酬が高い状態での行動をより頻繁にとるように方策関数を更新するため、報酬を最大化する方策関数を得ることができます。

$$
\nabla_\theta J(\theta) = \mathbb{E}{\pi\theta} \left[ \nabla_\theta \log \pi_\theta (s,a) Q^{\pi_\theta}(s,a) \right]
$$

ここで、 $$J(\theta)$$ は期待収益を表す関数、 $$\theta$$ は方策関数のパラメータ、 $$\pi_\theta$$ はパラメータ $$\theta$$ に依存する方策関数、 $$Q^{\pi_\theta}(s,a)$$ は状態 s で行動 a をとった場合の報酬の期待値を表す関数です。  
  
方策勾配法は、方策関数が任意の形状を持つことができるため、様々な問題に適用することができます。  
また、方策関数をニューラルネットワークで表現することができるため、大規模な問題にも適用できます。  
  
方策勾配法は、方策関数の更新によって報酬を最大化するため、強化学習の特性を十分に活かすことができます。  
しかし、方策関数の更新が局所解に陥ることがあり、最適解に収束するまでに時間がかかることがあります。  
また、方策勾配法は、報酬が連続的でない場合には適用できないことがあります。

### 参考

- [強化学習の基礎 - Qiita](https://qiita.com/icoxfog417/items/92ac72d4f4463f464fc3)
- [強化学習の基礎知識とアルゴリズムについて - RICOH](https://www.ricoh.co.jp/technology/techreport/2018/201811/pdf/002.pdf)
- [【JDLA E資格】強化学習 方策ベース（方策勾配法） - Qiita](https://qiita.com/fridericusgauss/items/aa5215c29646963bda29)

# 強化学習：方策勾配法

方策勾配法（Policy Gradient Method）は、強化学習における方策関数の最適化手法です。方策関数は、現在の状態に応じて、エージェントがどのような行動を取るかを決定する関数です。方策勾配法は、方策関数を微小に変更し、報酬を最大化するように更新していきます。

方策勾配法では、方策関数の勾配を計算し、その勾配に従って方策関数を更新します。勾配は、報酬と方策関数の微分によって計算されます。方策勾配法では、報酬が高い状態での行動をより頻繁にとるように方策関数を更新するため、報酬を最大化する方策関数を得ることができます。

方策勾配定理は、方策関数の勾配を計算するための基本的な式です。方策勾配定理は以下のように表されます。

ここで、 $J(\theta)$ は期待収益を表す関数、 $\theta$ は方策関数のパラメータ、 $\pi_\theta$ はパラメータ $\theta$ に依存する方策関数、 $Q^{\pi_\theta}(s,a)$ は状態 s で行動 a をとった場合の報酬の期待値を表す関数です。

方策勾配定理は、方策関数の微小な変化によって期待収益がどのように変化するかを表しています。方策勾配定理を用いることで、方策関数の勾配を計算し、その勾配に従って方策関数を更新することができます。

方策勾配定理は、方策勾配法の基本的な理論となっています。方策勾配定理を用いることで、方策関数を効率的に最適化することができます。

### 参考文献

- [強化学習の基礎 - Qiita](https://qiita.com/icoxfog417/items/92ac72d4f4463f464fc3)
- [強化学習の基礎知識とアルゴリズムについて - RICOH](https://www.ricoh.co.jp/technology/techreport/2018/201811/pdf/002.pdf)

[【JDLA E資格】強化学習 方策ベース（方策勾配法） - Qiita](https://qiita.com/fridericusgauss/items/aa5215c29646963bda29)

## A3C

Actorと呼ばれる機構を用いて方策 $$π_θ$$ に基づいて行動を選択し、Criticと呼ばれる機構にて選択された行動を評価する。  
[DQNを卒業してA3Cで途中挫折しないための7Tips - Qiita](https://qiita.com/yuishihara/items/2edad97148f09c282a9a)